{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f02ef741",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Thompson sampling - Bayesowskie podejmowanie decyzji\n",
    "subtitle: Matematyka w Analizie Sieci i Systemów\n",
    "author: Krzysztof Rusek\n",
    "numbersections: true\n",
    "lang: pl-PL\n",
    "theme: metropolis\n",
    "aspectratio: 169\n",
    "header-includes:\n",
    "  - \\usepackage{amsmath}\n",
    "  - \\usepackage{amssymb}\n",
    "  - \\usepackage{amsthm}\n",
    "  - \\usepackage{algorithm}\n",
    "  - \\usepackage{algorithmicx}\n",
    "  - \\usepackage{algpseudocode}\n",
    "  - \\usepackage{amsmath}\n",
    "  - \\usepackage{amssymb}\n",
    "  - \\usepackage{amsthm}\n",
    "  - \\usepackage{amsfonts}\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f78764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mwasis import *  # noqa: F403\n",
    "@tf.function(jit_compile=True)\n",
    "def f(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "redirect = FDRedirector(STDERR)\n",
    "redirect.start()\n",
    "f(tf.constant(1))\n",
    "redirect.stop();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419a475",
   "metadata": {},
   "source": [
    "# Wprowadzenie\n",
    "\n",
    "\n",
    "## Problem decyzji i uczenia\n",
    "\n",
    "Mamy do wyboru $N$ opcji. \n",
    "Każdy wybór daje jakąś nagrodę\n",
    "\n",
    "Jak optymalnie wykorzystać wiedzę historyczna do podejmowania przyszłych decyzji?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d65bb",
   "metadata": {},
   "source": [
    "# Przypomnienie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd810c",
   "metadata": {},
   "source": [
    "## Efekt danych\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95636c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "from sympy import symbols, Piecewise, integrate, simplify\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, Math, Markdown\n",
    "\n",
    "\n",
    "\n",
    "p = symbols('p')\n",
    "papr = Piecewise((4*p, p < 1/2), (4 - 4*p, p >= 1/2))\n",
    "\n",
    "pl_pr = sympy.plot(papr, (p, 0, 1),  xlabel='p', ylabel='f(p)', show=False)\n",
    "#pl.show()\n",
    "\n",
    "for no,nr in [(1,0),(3,1),(30,10)]:\n",
    "    #Dla otrzymania 1 orła i 3 reszek otrzymamy funkcjię likelihood postaci\n",
    "    display(Markdown(f'### Wypadło {no} orłow i {nr} reszek'))\n",
    "\n",
    "    likelihood = sympy.binomial(no+nr,nr)*(1 - p)**nr * p**no\n",
    "    pl_lik = sympy.plot(likelihood, (p, 0, 1),  xlabel='p', ylabel='f(data|p)', show=False);\n",
    "    #pl.show()\n",
    "\n",
    "\n",
    "    unnormalized_posterior = likelihood * papr\n",
    "    marginal_likelihood = integrate(unnormalized_posterior, (p, 0, 1))\n",
    "    post = unnormalized_posterior / marginal_likelihood\n",
    "\n",
    "    t = sympy.latex(sympy.simplify(post))\n",
    "    pl_pos=sympy.plot(post, (p, 0, 1),  xlabel='p', ylabel='f(p|data)', show=False) \n",
    "    #pl.show()\n",
    "\n",
    "    plotgrid = sympy.plotting.PlotGrid(2, 2, pl_pr,pl_pos,pl_lik, show=False,size=(7,4.0))\n",
    "    plotgrid.show()\n",
    "    display(Markdown('\\n --- \\n'))\n",
    "    display(Latex(r'$$'+t+r'$$'))\n",
    "    display(Markdown('\\n --- \\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9fe8dc",
   "metadata": {},
   "source": [
    "## Sprzężone a priori\n",
    "\n",
    "\\alert{$P(\\theta | D)$ i $P(\\theta )$ należą do tej samej rodziny}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355d049",
   "metadata": {},
   "source": [
    "\n",
    "## Problem formalnie\n",
    "\n",
    "**Multi-armed Bandit Problem (MAB)** \n",
    "\n",
    "<!-- - agent applies a sequence of actions $x_1, x_2, x_3,\\ldots \\in \\mathcal{X}$ to a system\n",
    "- applying action $x_t$, the agent observes an outcome\n",
    "$y_t$, which the system randomly generates according to a conditional probability measure $q_\\theta(\\cdot | x_t)$\n",
    "- The agent enjoys a reward $r_t = r(y_t)$, where $r$ is a known function.  \n",
    "- The agent is initially uncertain about the\n",
    "value of $\\theta$ and represents his uncertainty using a prior distribution $p$.\n",
    " -->\n",
    "\n",
    "- Agent stosuje sekwencję działań $x_1, x_2, x_3,\\ldots \\in \\mathcal{X}$ na systemie.\n",
    "- Stosując działanie $x_t$, agent obserwuje wynik $y_t$, który system generuje losowo zgodnie z warunkową miarą prawdopodobieństwa $q_\\theta(\\cdot | x_t)$.\n",
    "- Agent otrzymuje nagrodę $r_t = r(y_t)$, gdzie $r$ jest znaną funkcją.\n",
    "- Agent początkowo nie jest pewien wartości $\\theta$ i reprezentuje swoją niepewność, używając rozkładu początkowego $p$.\n",
    "\n",
    "## Nagroda\n",
    "\n",
    "$$\n",
    "E_{q_{\\hat{\\theta}}}[r(y_t) | x_t = x] = \\sum_o q_{\\hat{\\theta}}(o|x) r(o).\n",
    "$$\n",
    "\n",
    "\n",
    "## Strategia zachłanna\n",
    "\n",
    "Wybieramy opcje z najwieksza oczekiwana nagroda\n",
    "\n",
    "$$a = \\arg\\max_x E R_x$$\n",
    "\n",
    "### Problem\n",
    "\n",
    "Brak eksploracji\n",
    "\n",
    "## Strategia $\\epsilon$-zachłanna\n",
    "\n",
    "- Działamy zachlannie\n",
    "- z pr $\\epsilon$ wybieramy losowo\n",
    "\n",
    "### Problem\n",
    "\n",
    "Eksplorujemy, ale po długiej nauce dalej podejmujemy losowe decyzje\n",
    "\n",
    "## Bayesowskie podejmowanie decyzji \n",
    "\n",
    "Dzisiejsze a posteriori to jutrzejsze a priori\n",
    "\n",
    "### Sekwencja obserwacji\n",
    "\n",
    "$$x_1,x_2,x_3,\\ldots$$\n",
    "\n",
    "### Aktualizacja wiedzy\n",
    "\n",
    "Zakładamy klasę rozkładu $P$ z parametrami $\\theta$ i a priori $p_0(\\theta)$\n",
    "\n",
    "- Każda obserwacja uaktualnia nam wiedzę \n",
    "- $p_1(\\theta)\\sim p(x_1|\\theta)p_0(\\theta)$\n",
    "- $p(\\theta_i)= p(\\theta_{i-1}|x_i)$\n",
    "\n",
    "\n",
    "## Thompson sampling\n",
    "\n",
    "\\begin{algorithm}[H]\n",
    "\\begin{scriptsize}\n",
    "\\caption{{\\small $\\text{BernTS}(K, \\alpha, \\beta)$}}\\label{alg:BernoulliTS}\n",
    "\\begin{algorithmic}[1]\n",
    "\\For{$t=1,2,\\ldots $}\n",
    "\\State \\textcolor{blue}{\\#sample model:}\n",
    "\\For{$k=1, \\ldots, K$}\n",
    "\\State Sample $\\hat{\\theta}_k \\sim \\text{beta}(\\alpha_k, \\beta_k)$\n",
    "\\EndFor \\\\\n",
    "\\State \\textcolor{blue}{\\#select and apply action:}\n",
    "\\State $x_t \\leftarrow \\arg\\max_k \\hat{\\theta}_k$\n",
    "\\State Apply $x_t$ and observe $r_t$ \\\\\n",
    "\\State \\textcolor{blue}{\\#update distribution:}\n",
    "\\State $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow  (\\alpha_{x_t} + r_t, \\beta_{x_t} + 1-r_t)$\n",
    "\\EndFor\n",
    "\\end{algorithmic}\n",
    "\\end{scriptsize}\n",
    "\\end{algorithm}\n",
    "\n",
    "\\url{https://doi.org/10.48550/arXiv.1707.02038}\n",
    "\n",
    "## Strategia zachłanna\n",
    "\n",
    "\\begin{algorithm}[H]\n",
    "\\begin{scriptsize}\n",
    "\\caption{{\\small  $\\text{BernGreedy}(K, \\alpha, \\beta)$}}\\label{alg:BernoulliGreedy}\n",
    "\\begin{algorithmic}[1]\n",
    "\\For{$t=1,2,\\ldots $}\n",
    "\\State \\textcolor{blue}{\\#estimate model:}\n",
    "\\For{$k=1, \\ldots, K$}\n",
    "\\State $\\hat{\\theta}_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k)$\n",
    "\\EndFor \\\\\n",
    "\\State \\textcolor{blue}{\\#select and apply action:}\n",
    "\\State $x_t \\leftarrow \\arg\\max_k \\hat{\\theta}_k$\n",
    "\\State Apply $x_t$ and observe $r_t$ \\\\\n",
    "\\State \\textcolor{blue}{\\#update distribution:}\n",
    "\\State $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow  (\\alpha_{x_t} + r_t, \\beta_{x_t} + 1-r_t)$\n",
    "\\EndFor\n",
    "\\end{algorithmic}\n",
    "\\end{scriptsize}\n",
    "\\end{algorithm}\n",
    "\n",
    "\n",
    "# Wyniki numeryczne\n",
    "\n",
    "### Przykład{.example}\n",
    "Mamy trzy serwery, każdy obsługuje z rozkładem wykładniczym o nieznanej intensywności, chcemy wybrać najlepszy.\n",
    "\n",
    "## Zastosowanie\n",
    "\n",
    "* **Optymalizacja Wi-Fi:** Inteligentny wybór kanału i konfiguracji dla najlepszej wydajności sieci.\n",
    "* **Wybór konfiguracji systemu:** Skuteczne testowanie i wybieranie optymalnych ustawień w złożonych systemach.\n",
    "* **Uczenie ze wzmocnieniem (Reinforcement Learning):** Adaptacyjne algorytmy, które uczą się i dostosowują do dynamicznych środowisk.\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "- Proponujemy rozkład nagrody per ramie\n",
    "- Stosujemy sprzęzony rozkłd a priori\n",
    "- Aktualizujemy wiedze o parametrach\n",
    "- **Decyzja na podstawie próbki z a posteriori**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1addd942",
   "metadata": {},
   "source": [
    "# {.standout}\n",
    "\n",
    "Pytania"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
